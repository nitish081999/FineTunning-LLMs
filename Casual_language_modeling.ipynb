{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02499c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/nitish/anaconda3/lib/python3.10/site-packages (4.33.3)\n",
      "Requirement already satisfied: datasets in /home/nitish/anaconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: evaluate in /home/nitish/anaconda3/lib/python3.10/site-packages (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/nitish/anaconda3/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: filelock in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: requests in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nitish/anaconda3/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nitish/.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nitish/anaconda3/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/nitish/anaconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in /home/nitish/.local/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /home/nitish/anaconda3/lib/python3.10/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/nitish/.local/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pandas in /home/nitish/.local/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: multiprocess in /home/nitish/anaconda3/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/nitish/anaconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nitish/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/nitish/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nitish/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nitish/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nitish/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nitish/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nitish/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nitish/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nitish/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nitish/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nitish/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nitish/anaconda3/lib/python3.10/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nitish/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nitish/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nitish/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e6025",
   "metadata": {},
   "source": [
    "# Causal language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ceaf6",
   "metadata": {},
   "source": [
    "There are two types of language modeling, causal and masked. This guide illustrates causal language modeling. Causal language models are frequently used for text generation. You can use these models for creative applications like choosing your own text adventure or an intelligent coding assistant like Copilot or CodeParrot.\n",
    "Causal language modeling predicts the next token in a sequence of tokens, and the model can only attend to tokens on the left. This means the model cannot see future tokens. GPT-2 is an example of a causal language model.\n",
    "\n",
    "This guide will show you how to:\n",
    "\n",
    "Finetune DistilGPT2 on the r/askscience subset of the ELI5 dataset.\n",
    "Use your finetuned model for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce219117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d733f5071cf4713acf75427bf0b7a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdfa463",
   "metadata": {},
   "source": [
    "# Load ELI5 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d241c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4738e79d",
   "metadata": {},
   "source": [
    "# Split the datasetâ€™s train split into a train and test set with the train_test_split method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ac5ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c950bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '74wntd',\n",
       " 'title': 'Why does cold water help burns?',\n",
       " 'selftext': 'You would intuitively think that the burn damage is irreversible and not affected by the temperature of the skin after the burn.',\n",
       " 'category': 'Biology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers': {'a_id': ['do1mrrq', 'do1nu7x'],\n",
       "  'text': [\"There's a few reasons that cold water is recommended for burns. First, depending on the burn and what caused it, there can be ongoing damage occurring- kind of like how scrambled eggs keep cooking for a bit even after you take them out of the pan; residual heat can still be doing harm after you've moved away from the source of the damage. If you were burned by something other than direct heat, like an acid, chemical or other damaging material, the water can help remove any traces remaining, **DEPENDING ON THE CHEMICAL**. You may want to *avoid* water in some situations, but that's usually going to be in scenarios where you should be trained and wearing safety gear. Additionally, cold water helps soothe the pain by numbing it. Also, it can help reduce and prevent immediate swelling.\",\n",
       "   'Have you ever heard that you should take your steak off the barbecue just a little bit before it is as rare/well done as you like it? Because the heat within the meat will continue to cook it after it has been taken off the grill. Your meat works exactly the same. The cells have so much heat that they transfer it deeper into your tissue, further damaging it. Cold water helps take that heat away before it has a chance to conduct deeper into you. Running water is used because burns are playgrounds for bacteria, and standing water collects bacteria really well, but if the water is always running out of the tap, nothing can collect in it, and thus on you.'],\n",
       "  'score': [7, 4],\n",
       "  'text_urls': [[], []]},\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a51709",
   "metadata": {},
   "source": [
    "# Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579e91f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nitish/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25847416",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9957be18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_id': '74wntd',\n",
       " 'title': 'Why does cold water help burns?',\n",
       " 'selftext': 'You would intuitively think that the burn damage is irreversible and not affected by the temperature of the skin after the burn.',\n",
       " 'category': 'Biology',\n",
       " 'subreddit': 'explainlikeimfive',\n",
       " 'answers.a_id': ['do1mrrq', 'do1nu7x'],\n",
       " 'answers.text': [\"There's a few reasons that cold water is recommended for burns. First, depending on the burn and what caused it, there can be ongoing damage occurring- kind of like how scrambled eggs keep cooking for a bit even after you take them out of the pan; residual heat can still be doing harm after you've moved away from the source of the damage. If you were burned by something other than direct heat, like an acid, chemical or other damaging material, the water can help remove any traces remaining, **DEPENDING ON THE CHEMICAL**. You may want to *avoid* water in some situations, but that's usually going to be in scenarios where you should be trained and wearing safety gear. Additionally, cold water helps soothe the pain by numbing it. Also, it can help reduce and prevent immediate swelling.\",\n",
       "  'Have you ever heard that you should take your steak off the barbecue just a little bit before it is as rare/well done as you like it? Because the heat within the meat will continue to cook it after it has been taken off the grill. Your meat works exactly the same. The cells have so much heat that they transfer it deeper into your tissue, further damaging it. Cold water helps take that heat away before it has a chance to conduct deeper into you. Running water is used because burns are playgrounds for bacteria, and standing water collects bacteria really well, but if the water is always running out of the tap, nothing can collect in it, and thus on you.'],\n",
       " 'answers.score': [7, 4],\n",
       " 'answers.text_urls': [[], []],\n",
       " 'title_urls': ['url'],\n",
       " 'selftext_urls': ['url']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c69fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb385ffd",
   "metadata": {},
   "source": [
    "To apply this preprocessing function over the entire dataset, use the ðŸ¤— Datasets map method. You can speed up the map function by setting batched=True to process multiple elements of the dataset at once, and increasing the number of processes with num_proc. Remove any columns you donâ€™t need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc944d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e378a84c364e2b8e32bff283c61b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1212 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1312 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1793 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d655366a4f6467488e61703c6d8cfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1576 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1197 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1710 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2727 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe9c1bc",
   "metadata": {},
   "source": [
    "We can now use a second preprocessing function to\n",
    "\n",
    "concatenate all the sequences\n",
    "split the concatenated sequences into shorter chunks defined by block_size, which should be both shorter than the maximum input length and short enough for your GPU RAM.\n",
    "Copied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17241cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3dfe4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed582a790e6a40e99eb12bded3fdfaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d672236b0e2a48028232963c23f5edd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b2dd1b",
   "metadata": {},
   "source": [
    "Now create a batch of examples using DataCollatorForLanguageModeling. Itâ€™s more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4371b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56f1b8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4010f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "421f7105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3924' max='3924' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3924/3924 4:59:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.942500</td>\n",
       "      <td>3.847351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.846900</td>\n",
       "      <td>3.837938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.817200</td>\n",
       "      <td>3.837145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3924, training_loss=3.8751679244512935, metrics={'train_runtime': 17947.1366, 'train_samples_per_second': 1.749, 'train_steps_per_second': 0.219, 'total_flos': 1025230463041536.0, 'train_loss': 3.8751679244512935, 'epoch': 3.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=[\"none\"]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ef9ef63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='328' max='328' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [328/328 04:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a1885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca78e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ddf23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfaa7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
