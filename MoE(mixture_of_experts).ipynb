{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd12605c-b77d-4007-9e2f-fc44b565073d",
   "metadata": {},
   "source": [
    "The Mixture of Experts (MoE) architecture has become popular in recent months. This architecture offers an interesting tradeoff: higher performance at the cost of increased VRAM usage. While Mixtral and other MoE architectures are pre-trained from scratch, another method of creating MoE has recently appeared. Thanks to Arcee’s MergeKit library, we now have a new way of creating MoEs by ensembling several pre-trained models. These are often referred to as frankenMoEs or MoErges to distinguish them from the pre-trained MoEs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db85206-f3db-48bf-bf71-8527f84977db",
   "metadata": {},
   "source": [
    "# Introduction to MoEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43a32e-25fb-4604-85e7-44f6f531a37f",
   "metadata": {},
   "source": [
    "A Mixture of Experts is an architecture designed for improved efficiency and performance. It uses multiple specialized subnetworks, known as “experts.” Unlike dense models, where the entire network is activated, MoEs only activate relevant experts based on the input. This results in faster training and more efficient inference.\n",
    "\n",
    "There are two components at the core of an MoE model:\n",
    "\n",
    "**Sparse MoE Layers**: These replace the dense feed-forward network layers in the transformer architecture. Each MoE layer contains several experts, and only a subset of these experts are engaged for a given input.\n",
    "\n",
    "\n",
    "**Gate Network or Router**: This component determines which tokens are processed by which experts, ensuring that each part of the input is handled by the most suitable expert(s).\n",
    "In the following example, we show how a Mistral-7B block is transformed into an MoE block with a sparse MoE layer (feedforward network 1, 2, and 3) and a router. This example represents an MoE with three experts, where two are currently engaged (FFN 1 and FFN 3).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066eb25a-ec90-4146-8432-f6484791ba4d",
   "metadata": {},
   "source": [
    "MoEs also come with their own set of challenges, especially in terms of fine-tuning and memory requirements. The fine-tuning process can be difficult due to the model’s complexity, with the need to balance expert usage during training to properly train the gating weights to select the most relevant ones. In terms of memory, even though only a fraction of the total parameters are used during inference, the entire model, including all experts, needs to be loaded into memory, which requires high VRAM capacity.\n",
    "\n",
    "More specifically, there are two essential parameters when it comes to MoEs:\n",
    "\n",
    "Number of experts (num_local_experts): This determines the total number of experts in the architecture (e.g., 8 for Mixtral). The higher the number of experts, the higher the VRAM usage.\n",
    "Number of experts/token (num_experts_per_tok): This determines the number of experts that are engaged for each token and each layer (e.g., 2 for Mixtral). There is a tradeoff between a high number of experts per token for accuracy (but diminishing returns) vs. a low number for fast training and inference.\n",
    "Historically, MoEs have underperformed dense models. However, the release of Mixtral-8x7B in December 2023 shook things up and showed impressive performance for its size. Additionally, GPT-4 is also rumored to be an MoE, which would make sense as it would be a lot cheaper to run and train for OpenAI compared to a dense model. In addition to these recent excellent MoEs, we now have a new way of creating MoEs with MergeKit: frankenMoEs, also called MoErges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ff7b4-6d22-4ff1-ba51-a87dfc38df6c",
   "metadata": {},
   "source": [
    "# True MoEs vs. frankenMoEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aded1e7-d0e1-4b86-8a9c-5c96182d8f6c",
   "metadata": {},
   "source": [
    "The main difference between true MoEs and frankenMoEs is how they’re trained. In the case of true MoEs, the experts and the router are trained jointly. In the case of frankenMoEs, we upcycle existing models and initialize the router afterward.\n",
    "\n",
    "In other words, we copy the weights of the layer norm and self-attention layers from a base model, and then copy the weights of the FFN layers found in each expert. This means that besides the FFNs, all the other parameters are shared. This explains why Mixtral-8x7B with eight experts doesn’t have 8*7 = 56B parameters, but about 45B. This is also why using two experts per token gives the inference speed (FLOPs) of a 12B dense model instead of 14B.\n",
    "\n",
    "FrankenMoEs are about selecting the most relevant experts and initializing them properly. MergeKit currently implements three ways of initializing the routers:\n",
    "\n",
    "**Random:** Random weights. Be careful when using it as the same experts might be selected every time (it requires further fine-tuning or num_local_experts = num_experts_per_tok, which means you don’t need any routing).\n",
    "\n",
    "\n",
    "**Cheap embed:** It uses the raw embeddings of the input tokens directly and applies the same transformation across all layers. This method is computationally inexpensive and suitable for execution on less powerful hardware.\n",
    "\n",
    "\n",
    "**Hidden:** It creates hidden representations of a list of positive and negative prompts by extracting them from the last layer of the LLM. They are averaged and normalized to initialize the gates. More information about it is available on Charles Goddard’s blog.\n",
    "As you can guess, the “hidden” initialization is the most efficient to correctly route the tokens to the most relevant experts. In the next section, we will create our own frankenMoE using this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1015453-caa0-467f-8179-65b0563239dc",
   "metadata": {},
   "source": [
    "# Creating a frankenMoE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec247b9-e88e-498f-8436-c3f948b4b55c",
   "metadata": {},
   "source": [
    "To create our frankenMoE, we need to select n experts. In this case, we will rely on Mistral-7B thanks to its popularity and relatively small size. However, eight experts like in Mixtral is quite a lot, as we need to fit all of them in memory. For efficiency, I’ll only use four experts in this example, with two of them engaged for each token and each layer. In this case, we will end up with a model with 24.2B parameters instead of 4*7 = 28B parameters.\n",
    "\n",
    "Here, our goal is to create a well-rounded model that can do pretty much everything: write stories, explain articles, code in Python, etc. We can decompose this requirement into four tasks and select the best expert for each of them. This is how I decomposed it:\n",
    "\n",
    "**Chat model:** A general-purpose model that is used in most interactions. I used mlabonne/AlphaMonarch-7B, which perfectly satisfies the requirements.\n",
    "\n",
    "\n",
    "**Code model:** A model capable of generating good code. I don’t have a lot of experience with Mistral-7B-based code models, but I found beowolx/CodeNinja-1.0-OpenChat-7B particularly good compared to others.\n",
    "\n",
    "\n",
    "**Math model:** Math is tricky for LLMs, which is why we want a model specialized in math. Thanks to its high MMLU and GMS8K scores, I chose mlabonne/NeuralDaredevil-7B for this purpose.\n",
    "\n",
    "\n",
    "**Role-play model:** The goal of this model is to write high-quality stories and conversations. I selected SanjiWatsuki/Kunoichi-DPO-v2-7B because of its good reputation and high MT-Bench score (8.51 vs. 8.30 for Mixtral).\n",
    "Now that we’ve identified the experts we want to use, we can create the YAML configuration that MergeKit will use to create our frankenMoE. This uses the mixtral branch of MergeKit. You can find more information about how to write the configuration on this page. Here is our version:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0b6a67a-a418-49b3-95b9-bb85c034b3da",
   "metadata": {},
   "source": [
    "base_model: mlabonne/AlphaMonarch-7B\n",
    "experts:\n",
    "  - source_model: mlabonne/AlphaMonarch-7B\n",
    "    positive_prompts:\n",
    "    - \"chat\"\n",
    "    - \"assistant\"\n",
    "    - \"tell me\"\n",
    "    - \"explain\"\n",
    "    - \"I want\"\n",
    "  - source_model: beowolx/CodeNinja-1.0-OpenChat-7B\n",
    "    positive_prompts:\n",
    "    - \"code\"\n",
    "    - \"python\"\n",
    "    - \"javascript\"\n",
    "    - \"programming\"\n",
    "    - \"algorithm\"\n",
    "  - source_model: SanjiWatsuki/Kunoichi-DPO-v2-7B\n",
    "    positive_prompts:\n",
    "    - \"storywriting\"\n",
    "    - \"write\"\n",
    "    - \"scene\"\n",
    "    - \"story\"\n",
    "    - \"character\"\n",
    "  - source_model: mlabonne/NeuralDaredevil-7B\n",
    "    positive_prompts:\n",
    "    - \"reason\"\n",
    "    - \"math\"\n",
    "    - \"mathematics\"\n",
    "    - \"solve\"\n",
    "    - \"count\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "ddb2f80b-70a7-49d8-b62d-b93d31c68a1b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
